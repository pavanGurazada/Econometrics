---
title: "Applied Regression Modeling, with a twist"
author: Pavan Gurazada
date: February 2018
output: github_document
---

Global options and package dependencies

```{r}
library(tidyverse)
library(caret)
library(arm)
library(foreign)

library(keras)

set.seed(20130810)
theme_set(theme_classic())

```

In this workbook we go through some of the examples and exercises from the classic, [Gelman and Hill (2007)](https://www.amazon.com/Analysis-Regression-Multilevel-Hierarchical-Models/dp/052168689X/ref=cm_cr_arp_d_product_top?ie=UTF8).

The Stan group has posted most of the associated R code for the book [here](https://github.com/stan-dev/example-models/tree/master/ARM).

# Chapter 3 & 4

Load up the data for the chapter and take a look.
```{r}
load("data/gelman/kidiq.rda")
```

```{r}
glimpse(kidiq)
```

The problem is to predict the IQ score of kids depending on the features associated with the mother.

We start by splitting the data out.

```{r}
trainingRows <- createDataPartition(kidiq$kid_score, p = 0.8, list = FALSE)

kidiqTrain <- kidiq[trainingRows, ]
kidiqTest <- kidiq[-trainingRows, ]
```

Let us look at the distribution of the outcome.

```{r}
qplot(kidiqTrain$kid_score, geom = "histogram", binwidth = 10) + 
  labs(x = "Kid score",
       title = "Distribution of outcome (training data)")
```

Seems okay, with no abnormal skew. Lets run a simple linear regression.

```{r}
kidiqFit1 <- train(kid_score ~ .,
                  data = kidiqTrain,
                  method = "lm",
                  preProcess = c("center", "scale"),
                  trControl = trainControl(method = "repeatedcv",
                                           number = 10,
                                           repeats = 5))
```

```{r}
summary(kidiqFit1)
```

Pretty pathetic $R^2$. Lets look at the cross validation scores.

```{r}
print(kidiqFit1$results)
```

Hopeless. Lets try interactions, throw out mom_age as it does not seem to contribute.

```{r}
kidiqFit2 <- train(kid_score ~ mom_hs * mom_iq,
                   data = kidiqTrain,
                   method = "lm",
                   preProcess = c("center", "scale"),
                   trControl = trainControl(method = "repeatedcv",
                                            number = 10,
                                            repeats = 5))
```

```{r}
summary(kidiqFit2)
```

Marginally better. Now to the cross validation results.

```{r}
print(kidiqFit2$results)
```

Still pathetic.

Let us try ridge regression and elastic net.

```{r}
kidiqFit3 <- train(kid_score ~ mom_hs * mom_iq,
                   data = kidiqTrain,
                   method = "ridge",
                   preProcess = c("center", "scale"),
                   tuneGrid = expand.grid(lambda = c(0, 0.001, 0.01, 0.1)),
                   trControl = trainControl(method = "repeatedcv",
                                            number = 10,
                                            repeats = 5))

kidiqFit4 <- train(kid_score ~ mom_hs * mom_iq,
                   data = kidiqTrain,
                   method = "glmnet",
                   preProcess = c("center", "scale"),
                   tuneGrid = expand.grid(lambda = c(0, 0.001, 0.01, 0.1),
                                          alpha = seq(0.05, 1, length.out = 20)),
                   trControl = trainControl(method = "repeatedcv",
                                            number = 10,
                                            repeats = 5))
```

```{r}
print(kidiqFit3$results)
```

```{r}
print(kidiqFit4$results)
```


```{r}
coef(kidiqFit4$finalModel, kidiqFit4$bestTune$lambda)
```


Checking the real-world performance.

```{r}
yPredict <- predict(kidiqFit3, kidiqTest)
RMSE(yPredict, kidiqTest$kid_score)
```

Not worth losing the interpretability of coefficients by moving beyond the simple linear regression. How important is significance of the features if the predictive power is not great?

# Chapter 5: Logistic regression

```{r}
load("data/gelman/wells.rda")

glimpse(wells)
```

A research team measured all wells in Bangladesh and advised people near the wells about the levels of Arsenic in them. People around wells with high levels of arsenic were advised to switch to nearby private or community wells or to new wells. They returned after a few years and observed the switching decision of a sample of households and measured them on a few features.

Begin by observing the distribution of the outcome for signs of a class imbalance.

```{r}
qplot(wells$switch, geom = "bar") +
  labs(x = "Switched?",
       title = "Distribution of outcome")
```

No severe class imbalance. Let us split the data.

```{r}
trainingRows <- createDataPartition(wells$switch, p = 0.8, list = FALSE)
wellsTrain <- wells[trainingRows, ]
wellsTest <- wells[-trainingRows, ]
```

Repeat distribution of outcome on the training data.

```{r}
qplot(wellsTrain$switch, geom = "bar") +
  labs(x = "Switched?",
       title = "Distribution of outcome (training set)")
```

Lets fit a logistic regression.

```{r}
wellsFit1 <- train(factor(switch) ~ dist + arsenic + assoc + educ + dist:arsenic,
                   data = wellsTrain,
                   method = "glm",
                   preProcess = c("center", "scale"),
                   trControl = trainControl(method = "repeatedcv",
                                            number = 10,
                                            repeats = 5))

summary(wellsFit1)
```

Lets look at the cross-validation score.

```{r}
print(wellsFit1$results)
```

Not that great accuracy. Try removing the association variable.

```{r}
wellsFit2 <- train(factor(switch) ~ dist + arsenic + educ + dist:arsenic,
                   data = wellsTrain,
                   method = "glm",
                   preProcess = c("center", "scale"),
                   trControl = trainControl(method = "repeatedcv",
                                            number = 10,
                                            repeats = 5))

summary(wellsFit2)
```

Cross-validation accuracy:

```{r}
print(wellsFit2$results)
```

Add more interactions and log(arsenic) following the logic in pages 95 - 97.

```{r}
wellsFit3 <- train(factor(switch) ~ dist + log(arsenic) + educ + dist:log(arsenic) +
                                    dist:educ + log(arsenic):educ,
                   data = wellsTrain,
                   method = "glm",
                   preProcess = c("center", "scale"),
                   trControl = trainControl(method = "repeatedcv",
                                            number = 10,
                                            repeats = 5))

summary(wellsFit3)
```

```{r}
print(wellsFit3$results)
```
Slight jump in accuracy.

```{r}
confusionMatrix(wellsFit3)
```

In principle, the accuracy of the logistic regression should be no greater than a neural network with one hidden layer.

```{r}
nnFit <- keras_model_sequential()

nnFit %>% layer_dense(units = 1, activation = "relu", input_shape = c(4)) %>% 
          layer_dropout(rate = 0.4) %>% 
          layer_dense(units = 1, activation = "sigmoid")

summary(nnFit)
```

```{r}
nnFit %>% compile(loss = "categorical_crossentropy",
                  optimizer = optimizer_rmsprop(),
                  metrics = c("accuracy"))

history <- nnFit %>% fit(wellsTrain[, 2:5], wellsTrain$switch,
                         epochs = 30, batch_size = 100,
                         validation_split = 0.2)

```

```{r}
nnetFit <- train(factor(switch) ~ dist + log(arsenic) + educ + dist:log(arsenic) +
                                    dist:educ + log(arsenic):educ,
                 data = wellsTrain,
                 method = "nnet",
                 preProcess = c("center", "scale"),
                 tuneGrid = expand.grid(decay = c(0, 0.01, 0.1),
                                        size = c(1:2)),
                 trControl = trainControl(method = "cv", 
                                          allowParallel = TRUE),
                 linout = TRUE,
                 trace = FALSE,
                 maxit = 500)


```

# Chapter 6: Generalized linear models (GLM)

All GLMs transform a linear combination of predictors to map to the outcome. Unmodeled variation between the predicted and observed data follows a chosen probability distribution. Together these parameters define the kind of GLM, e. g., Logistic or Ordered Multinomial or Poisson.

## Example 1: Predicting presidential elections

```{r}
nes <- read.dta("data/gelman/nes5200_processed_voters_realideo.dta")
glimpse(nes)
```

*Question:* Predict presidential vote based on ideology and demographics using a multinomial logit.

Lets select the relevant subset of the data.

```{r}
nesPresVote <- nes %>%  dplyr::select(gender, race, educ1, income, religion, ideo, presvote) 
summary(nesPresVote)
```

Bunch of missing values, especially for ideology. The outcome is missing on a bunch too. Lets take out count the number of rows that have at least one missing value. 

```{r}
glimpse(nesPresVote)
missingRows <- as.vector(rowSums(is.na(nesPresVote)) > 0)
sum(missingRows)

nesClean <- na.omit(nesPresVote)
glimpse(nesClean)
summary(nesClean)
```

Thats a lot of data that goes out. There are a bunch of factor levels with no data, we need to drop them.

```{r}
nesClean <- droplevels(nesClean)
glimpse(nesClean)
```

Lets split the data. 

```{r}
trainingRows <- createDataPartition(nesClean$presvote, p = 0.8, list = FALSE)
nesCleanTrain <- nesClean[trainingRows, ]
nesCleanTest <- nesClean[-trainingRows, ]

summary(nesCleanTrain)
```

```{r}
mnLogit <- train(presvote ~ .,
                 data = nesCleanTrain,
                 method = "polr",
                 trControl = trainControl(method = "repeatedcv",
                                          number = 10, 
                                          repeats = 5))
```


```{r}
summary(mnLogit)
```

```{r}
print(mnLogit$results)
```

Thats a good accuracy given the amount and kind of  data we have!

```{r}
confusionMatrix(mnLogit)
```

Lets predict on the test data.

```{r}
presVotePred <- predict(mnLogit, nesCleanTest)
confusionMatrix(presVotePred, nesCleanTest$presvote)
```

## Predicting oscar awards

```{r}
oscars <- read_csv("data/gelman/oscars.csv")
glimpse(oscars)
```

No missing values

```{r}
sum(is.na(oscars))
```

Lazy model

```{r}
trainingRows <- createDataPartition(oscars$Ch, p = 0.8, list = FALSE)
oscarsTrain <- oscars[trainingRows, ]
oscarsTest <- oscars[-trainingRows, ]

oscarMultiNomFit <- train(factor(Ch) ~ .,
                          data = oscarsTrain,
                          method = "lm",
                          trControl = trainControl(method = "repeatedcv",
                                                   number = 10, 
                                                   repeats = 5))
```

